# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

AKRIN AI Customer Service Chatbot - A high-performance AI-powered customer service chatbot for IT service providers, implementing a sophisticated microservices architecture inspired by Intercom's Fin.ai.

## Critical Deployment Context

This project is actively being deployed to Render.com. Current deployment status:
- Python version: 3.11.0 (must use full version, not 3.11)
- Deployment issues resolved: Rasa removed (incompatible with Python 3.11), PyJWT added
- Database: Supabase PostgreSQL (free tier)
- Vector stores temporarily disabled until Pinecone is configured

## Essential Commands

### Development
```bash
# Install dependencies
make install

# Run development server (requires Docker for local services)
make dev

# Run without Docker (uses SQLite)
uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000

# Run tests
make test
pytest tests/ -v --cov=src

# Linting and formatting
make lint       # Run flake8 and mypy
make format     # Run black and isort
```

### Deployment
```bash
# Build for production
make build

# Deploy to Render (auto-deploys on push to main)
git push origin main

# Initialize database after deployment
python scripts/init_db.py
```

## Architecture Overview

### Core Components

1. **API Layer** (`src/api/`)
   - FastAPI application with WebSocket support
   - RESTful endpoints + real-time chat via WebSocket
   - Static file serving for chat widget and agent dashboard

2. **Core Engine** (`src/core/`)
   - `nlp_engine.py`: Intent recognition and dialogue management
   - `database.py`: Database abstraction (PostgreSQL/SQLite)
   - `config.py`: Settings management via Pydantic

3. **Knowledge System** (`src/knowledge/`)
   - RAG module for retrieval-augmented generation
   - Vector store integration (Pinecone - currently disabled)
   - Local fallback implementation available

4. **Integration Points**
   - Multiple LLM providers: OpenAI, Anthropic, Google Gemini
   - Database: PostgreSQL (Supabase) for production, SQLite for development
   - Monitoring: Prometheus metrics exposed at `/metrics`

### Key Design Decisions

1. **Database Strategy**: Dual support for PostgreSQL (production) and SQLite (development) via factory pattern
2. **Async Architecture**: Full async/await pattern using FastAPI and asyncpg
3. **WebSocket Implementation**: Real-time bidirectional communication for chat
4. **Static File Serving**: Chat widget and agent dashboard served directly from FastAPI

## Deployment Configuration

### Render.yaml Structure
- Single web service configuration
- Environment variables marked as `sync: false` must be set in Render dashboard
- Health check at `/api/health`
- Auto-deploy enabled from main branch

### Required Environment Variables
```
DATABASE_URL          # PostgreSQL connection string (Supabase)
OPENAI_API_KEY       # OpenAI API key
ANTHROPIC_API_KEY    # Anthropic API key (optional)
PINECONE_API_KEY     # Pinecone API key (optional for now)
JWT_SECRET_KEY       # Auto-generated by Render
```

## Current Limitations

1. **Vector Store**: Pinecone integration disabled - using simple database search
2. **Heavy ML Libraries**: Removed (spacy, transformers, rasa) - using OpenAI for all NLP
3. **Message Queue**: Kafka integration commented out - not needed for MVP
4. **Monitoring**: Elasticsearch disabled - using basic logging only

## WebSocket Endpoints

- Customer Chat: `/ws/chat/{session_id}`
- Agent Dashboard: `/ws/agent/{agent_id}`

Both auto-detect production vs development environment for proper WSS/WS protocol.

## Static Files

- Chat Widget: `/static/chat-widget.html`
- Agent Dashboard: `/static/agent-dashboard.html`

These files contain self-configuring WebSocket connections that detect the environment.

## Database Schema

The application auto-creates these tables on startup:
- `users`: External user tracking
- `conversations`: Chat session management
- `messages`: Chat history with intent/confidence tracking
- `knowledge_articles`: Knowledge base for RAG

## Testing Strategy

- Unit tests in `tests/` directory
- Use pytest with coverage reporting
- Mock external services (LLMs, vector stores) in tests
- Integration tests for API endpoints

## Deployment Checklist

When deploying updates:
1. Ensure all dependencies in requirements.txt are Python 3.11 compatible
2. Test locally with `python startup.py`
3. Verify environment variables are set in Render
4. Check that static files are accessible after deployment
5. Monitor logs for any import errors or missing dependencies